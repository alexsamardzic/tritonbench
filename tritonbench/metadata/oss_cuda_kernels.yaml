addmm:
  aten_addmm:
    tags:
    - aten
  pt2_addmm_maxautotune:
    tags:
    - pt2
  pt2_triton_matmul:
    tags:
    - pt2
  streamk_addmm:
    kernels:
    - streamk_cuda_gemm
    tags:
    - triton
  triton_addmm:
    kernels:
    - _addmm_fwd
    tags:
    - triton
bf16xint16_gemm:
  bf16xbf16:
    kernels:
    - bf16xbf16_matmul_kernel
    tags:
    - triton
  bf16xint16:
    kernels:
    - bf16xint16_matmul_kernel
    tags:
    - triton
  bf16xint16_casted:
    kernels:
    - bf16xbf16_matmul_kernel
    tags:
    - triton
  eager_bf16xbf16:
    tags:
    - aten
  torch_compile_bf16xbf16:
    tags:
    - pt2
blackwell_attentions:
  aten:
    tags:
    - aten
  cudnn_sdpa:
    tags: []
  cutedsl_blackwell:
    tags: []
  cutlass_blackwell:
    tags:
    - xformers
    - cutlass
  flash_v2:
    tags: []
  flex_attention:
    tags:
    - pt2
  gluon_blackwell_tutorial_fwd:
    kernels: []
    tags:
    - gluon
  gluon_blackwell_tutorial_persistent_fwd:
    kernels: []
    tags:
    - gluon
  sdpa:
    tags:
    - pt2
  tlx_blackwell_ws_pipelined_fwd:
    tags:
    - tlx
  tlx_blackwell_ws_pipelined_persistent_fwd:
    tags:
    - tlx
  triton_tutorial_flash_dp_blackwell:
    kernels:
    - _attn_fwd
    - _attn_fwd_persist
    tags:
    - triton
  triton_tutorial_flash_dp_persistent_blackwell:
    kernels:
    - _attn_fwd
    - _attn_fwd_persist
    tags:
    - triton
  triton_tutorial_flash_v2_blackwell:
    kernels:
    - _attn_fwd
    - _attn_fwd_persist
    - _attn_bwd_preprocess
    - _attn_bwd
    tags:
    - triton
  triton_tutorial_flash_v2_persistent_blackwell:
    kernels:
    - _attn_fwd
    - _attn_fwd_persist
    - _attn_bwd_preprocess
    - _attn_bwd
    tags:
    - triton
  triton_tutorial_flash_v2_tma_ws_persistent_blackwell:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
  xformers_splitk:
    tags:
    - xformers
cross_entropy:
  cross_entropy_loss:
    tags: []
  liger_cross_entropy_loss:
    tags:
    - liger
    - triton
  torch_compile_cross_entropy_loss:
    tags:
    - pt2
decoding_attention:
  aiter_paged_fp8kv:
    tags: []
  fa2_kvcache:
    tags: []
  fa2_mha_varlen_fwd:
    tags:
    - xformers
  fa3_kvcache_fp8qkv:
    tags: []
  fa3_kvcache_gqa_heuristic:
    tags: []
  fa3_mha_varlen_fwd:
    tags:
    - xformers
  mslk_gqa_fp8kv:
    kernels:
    - torch.ops.mslk.gqa_attn_splitk
    tags:
    - native_custom_ops
    - mslk
  flash_cute_dsl:
    tags: []
  triton_splitk:
    tags:
    - xformers
  triton_splitk_fp8kv:
    tags:
    - xformers
embedding:
  liger_embedding:
    kernels:
    - embedding_forward_kernel
    tags:
    - triton
    - liger
  torch_compile_embedding:
    tags:
    - pt2
  torch_embedding:
    tags:
    - aten
flash_attention:
  aten:
    tags:
    - aten
  cudnn:
    tags:
    - aten
  flash_v2:
    tags: []
  flash_v3:
    tags: []
  flex_attention:
    tags:
    - pt2
  pallas:
    tags: []
  sdpa:
    tags:
    - pt2
  tile:
    kernels:
    - tritonbench.operators.flash_attention.tilelang_mha
    tags:
    - tilelang
  tk:
    tags: []
  triton_tutorial_flash_v2:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
  triton_tutorial_flash_v2_tma:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
  triton_tutorial_flash_v2_tma_ws:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
  triton_tutorial_flash_v2_tma_ws_persistent:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
  triton_tutorial_flash_v2_ws:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
  xformers:
    tags:
    - xformers
  xformers_splitk:
    tags:
    - xformers
fp8_attention:
  colfax_fmha:
    kernels:
    - torch.ops.cutlass.fmha_forward_pipeline
    tags:
    - native_custom_ops
  triton_flash_v2:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
  triton_flash_v2_tma:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
  triton_flash_v2_ws:
    kernels:
    - _attn_fwd_tma_ws_persistent
    - _attn_fwd_tma_ws_persistent_with_dp
    - _attn_bwd
    - _attn_fwd_base_opt
    - _attn_bwd_preprocess
    - _attn_fwd_ws
    - _attn_fwd_tma_unified
    tags:
    - triton
fp8_gemm:
  pt2_fp8_gemm:
    tags:
    - pt2
  torch_fp8_gemm:
    tags: []
  triton_fp8_gemm:
    kernels:
    - matmul_kernel
    tags:
    - triton
  triton_persistent_fp8_gemm:
    kernels:
    - matmul_kernel_persistent
    tags:
    - triton
  triton_tma_persistent_fp8_gemm:
    kernels:
    - matmul_kernel_tma_persistent
    tags:
    - triton
fp8_gemm_blockwise:
  _cutlass:
    kernels:
    - torch.ops.mslk.f8f8bf16_blockwise
    tags:
    - native_custom_ops
    - mslk
    - cutlass
  _triton:
    kernels:
    - _kernel_matmul_fp8_block_slowacc
    - _kernel_matmul_fp8_block_fastacc
    tags:
    - triton
fp8_gemm_rowwise:
  _aoti_fp8_triton_mm:
    kernels:
    - _aoti_fp8_triton_mm
    tags:
    - triton
  _cublas:
    kernels:
    - torch.ops.mslk.f8f8bf16_cublas
    - torch.ops.mslk.f8f8bf16_cublas
    tags:
    - native_custom_ops
    - mslk
  _cutlass_or_ck:
    kernels:
    - torch.ops.mslk.f8f8bf16_rowwise
    - torch.ops.mslk.f8f8bf16_rowwise
    tags:
    - native_custom_ops
    - mslk
    - cutlass
  _triton:
    kernels:
    - <torch._library.capture_triton(_kernel_matmul_fp8_row_tma_persistent_ws_cooperative)>
    - <torch._library.capture_triton(_kernel_matmul_fp8_row_imprecise_acc)>
    - <torch._library.capture_triton(_kernel_matmul_fp8_row_tma_persistent)>
    - <torch._library.capture_triton(_kernel_matmul_fp8_row_no_fast_acc)>
    - <torch._library.capture_triton(_kernel_matmul_fp8_row)>
    - <torch._library.capture_triton(_kernel_matmul_fp8_row_non_persistent)>
    tags:
    - triton
fp8_gemm_rowwise_grouped:
  _cutlass_or_ck:
    kernels:
    - torch.ops.mslk.f8f8bf16_rowwise_grouped_stacked
    tags:
    - native_custom_ops
    - mslk
    - cutlass
  _triton:
    kernels:
    - fn
    tags:
    - triton
  eager_fp8_gemm_rowwise_grouped:
    tags:
    - aten
fused_linear_cross_entropy:
  liger_lm_head_ce:
    tags:
    - liger
    - triton
  torch_compile_fused_linear_cross_entropy:
    tags:
    - pt2
  torch_lm_head_ce:
    tags: []
fused_linear_jsd:
  liger_lm_head_jsd:
    tags:
    - liger
    - triton
  torch_compile_lm_head_jsd:
    tags:
    - pt2
  torch_lm_head_jsd:
    tags: []
gather_gemv:
  eager_gather_gemv:
    tags:
    - aten
  torch_compile_gather_gemv:
    tags:
    - pt2
  triton_gather_gemv:
    kernels:
    - torch.ops.inductor._reinterpret_tensor
    - triton_red_fused_mv_0
    tags:
    - native_custom_ops
    - triton
gdn_fwd_h:
  compile:
    tags:
    - pt2
  eager:
    tags:
    - aten
  fla:
    tags: []
  tilelang:
    tags: []
gdpa:
  eager_gdpa:
    tags:
    - aten
  gdpa:
    kernels:
    - <torch._library.capture_triton(kernel_fn)>
    tags:
    - triton
  gdpa_opt:
    kernels:
    - <torch._library.capture_triton(kernel_fn)>
    tags:
    - triton
  gdpa_opt_sorted:
    kernels:
    - <torch._library.capture_triton(kernel_fn)>
    tags:
    - triton
  tlx_gdpa:
    tags:
    - tlx
  torch_compile_gdpa:
    tags:
    - pt2
geglu:
  liger_geglu:
    tags:
    - liger
    - triton
  torch_compile_geglu:
    tags:
    - pt2
  torch_geglu:
    tags: []
gemm:
  aten_matmul:
    tags:
    - aten
  aten_tunableop_matmul:
    tags:
    - aten
  hstu_triton_matmul:
    tags: []
  matmul_decompose_k:
    tags:
    - pt2
  matmul_partition_k:
    kernels:
    - _reduce
    - _matmul_partition_k
    tags:
    - triton
  pt2_cutlass_matmul:
    tags:
    - pt2
    - cutlass
  pt2_matmul_maxautotune:
    tags:
    - pt2
  pt2_triton_matmul:
    tags:
    - pt2
  streamk_matmul:
    kernels:
    - streamk_amd_gemm
    tags:
    - triton
  triton_ops_matmul:
    kernels:
    - _splitk_kernel
    tags:
    - triton
  triton_persistent_matmul:
    tags: []
  triton_tma_persistent_cached_matmul:
    tags: []
  triton_tma_persistent_matmul:
    tags: []
  triton_tutorial_matmul:
    kernels:
    - matmul_kernel
    tags:
    - triton
grouped_gemm:
  aten_grouped_mm:
    tags:
    - aten
  cutedsl_grouped_mm:
    kernels:
    - tritonbench.operators.grouped_gemm.cutedsl.kernels.compile_cutedsl_grouped_gemm
    tags:
    - cutedsl
  naive:
    tags: []
  precompiled_cutedsl_grouped_mm:
    kernels:
    - tritonbench.operators.grouped_gemm.cutedsl.kernels.compile_cutedsl_grouped_gemm
    tags:
    - cutedsl
  precompiled_cutedsl_grouped_mm_tuned:
    kernels:
    - ns
    tags:
    - triton
  preprocessed_aten_grouped_mm:
    tags:
    - aten
  preprocessed_pt2_cute_grouped_mm:
    tags:
    - pt2
  preprocessed_pt2_triton_grouped_mm:
    tags:
    - pt2
  tlx_grouped_gemm:
    kernels:
    - grouped_matmul_tlx_kernel
    tags:
    - triton
    - tlx
  torch_compile_grouped_gemm:
    tags:
    - pt2
  triton_grouped_gemm:
    kernels:
    - grouped_matmul_kernel
    tags:
    - triton
int4_gemm:
  eager_int4_gemm:
    tags:
    - aten
  preprocessed_eager_int4_gemm:
    tags:
    - aten
  preprocessed_torch_compile_int4_gemm:
    tags:
    - pt2
  preprocessed_triton_int4_gemm:
    kernels:
    - matmul_kernel
    tags:
    - triton
  torch_compile_int4_gemm:
    tags:
    - pt2
  triton_int4_gemm:
    kernels:
    - matmul_kernel
    tags:
    - triton
jagged_layer_norm:
  torch_compile_nested_tensor_integration:
    tags:
    - pt2
  torch_jagged_layer_norm_torch_sum:
    kernels:
    - torch.ops.aten._jagged_to_padded_dense_forward
    - torch.ops.aten._jagged_to_padded_dense_forward
    - torch.ops.aten._padded_dense_to_jagged_forward
    tags:
    - native_custom_ops
  torch_jagged_layer_norm_unbind_torch_layer_norm:
    tags:
    - aten
jagged_mean:
  torch_compile_jagged_mean_torch_nanmean:
    tags:
    - pt2
  torch_compile_jagged_mean_torch_sum:
    tags:
    - pt2
  torch_compile_jagged_mean_unbind_torch_mean:
    tags:
    - pt2
  torch_compile_nested_tensor_integration:
    tags:
    - pt2
  torch_jagged_mean_torch_nanmean:
    kernels:
    - torch.ops.aten._jagged_to_padded_dense_forward
    tags:
    - native_custom_ops
  torch_jagged_mean_torch_sum:
    kernels:
    - torch.ops.aten._jagged_to_padded_dense_forward
    tags:
    - native_custom_ops
  torch_jagged_mean_unbind_torch_mean:
    tags: []
  triton_jagged_mean_simple_fused:
    kernels:
    - triton_jagged_mean_kernel_simple_fused_sum_then_buffer
    tags:
    - triton
  triton_jagged_mean_variable_length_loop:
    kernels:
    - triton_jagged_mean_kernel_variable_length_loop_sum_then_buffer
    tags:
    - triton
jagged_softmax:
  torch_compile_jagged_softmax_torch_sum:
    tags:
    - pt2
  torch_compile_nested_tensor_integration:
    tags:
    - pt2
  torch_jagged_softmax_torch_sum:
    kernels:
    - torch.ops.aten._jagged_to_padded_dense_forward
    - torch.ops.aten._padded_dense_to_jagged_forward
    tags:
    - native_custom_ops
  torch_jagged_softmax_unbind_torch_softmax:
    tags: []
  triton_jagged_softmax_simple_fused:
    kernels:
    - triton_jagged_softmax_kernel_simple_fused_buffer_then_sum
    tags:
    - triton
  triton_jagged_softmax_variable_length_loop:
    kernels:
    - triton_jagged_softmax_kernel_variable_length_loop_buffer_then_sum
    tags:
    - triton
jagged_sum:
  torch_compile_nested_tensor_integration:
    tags:
    - pt2
  torch_jagged_sum_no_pad:
    tags: []
  torch_jagged_sum_pad:
    kernels:
    - torch.ops.aten._jagged_to_padded_dense_forward
    tags:
    - native_custom_ops
  triton_jagged_sum_no_pad_simple_fused:
    kernels:
    - triton_jagged_sum_kernel_simple_fused_sum_then_buffer
    tags:
    - triton
  triton_jagged_sum_no_pad_variable_length_loop:
    kernels:
    - triton_jagged_sum_kernel_variable_length_loop_sum_then_buffer
    tags:
    - triton
jsd:
  liger_jsd:
    tags:
    - liger
    - triton
  torch_compile_jsd:
    tags:
    - pt2
  torch_jsd:
    tags: []
kl_div:
  liger_kl_div:
    tags:
    - liger
    - triton
  torch_compile_kl_div:
    tags:
    - pt2
  torch_kl_div:
    tags: []
launch_latency:
  nop_inductor_kernel:
    tags: []
  nop_python_function:
    tags: []
  nop_triton_compiled_kernel_run:
    tags:
    - pt2
  nop_triton_kernel:
    kernels:
    - nop_kernel
    tags:
    - triton
layer_norm:
  liger_layer_norm:
    kernels:
    - _layer_norm_forward_kernel
    tags:
    - triton
    - liger
  quack_layer_norm:
    tags: []
  torch_compile_layer_norm:
    tags:
    - pt2
  torch_layer_norm:
    tags:
    - aten
  triton_fused_layer_norm:
    kernels:
    - _layer_norm_fwd_fused_no_bias
    - _layer_norm_bwd_dx_fused
    tags:
    - triton
  triton_layer_norm:
    kernels:
    - _layer_norm_bwd_dwdb
    - _layer_norm_fwd_fused
    - _layer_norm_bwd_dx_fused
    tags:
    - triton
low_mem_dropout:
  eager_dropout:
    tags:
    - aten
  seeded_dropout:
    kernels:
    - _seeded_triton_dropout
    tags:
    - triton
  torch_compile_dropout:
    tags:
    - pt2
  triton_dropout:
    kernels:
    - _triton_dropout
    tags:
    - triton
mamba2_chunk_scan:
  compile:
    tags:
    - pt2
  eager:
    tags:
    - aten
  mamba_ssm:
    tags: []
  tilelang:
    tags: []
mamba2_chunk_state:
  compile:
    tags:
    - pt2
  eager:
    tags:
    - aten
  mamba_ssm:
    tags: []
  mamba_ssm_ref:
    tags: []
  tilelang:
    tags: []
mixed_gemm:
  aten_bf16_bf16:
    tags:
    - aten
  cutlass_w2a16:
    kernels:
    - torch.ops.mixed_gemm.w2a16_gemm
    tags:
    - native_custom_ops
    - cutlass
  machete_w4a16:
    tags: []
  marlin_w4a16:
    kernels:
    - torch.ops.marlin.marlin_gemm
    tags:
    - native_custom_ops
  tinny_w4a16:
    kernels:
    - torch.ops.tinygemm.convert_matrix_to_m16n8k16_Aint4_layout
    - torch.ops.tinygemm.tinygemm_y_f16RM_x_f16RM_w_int4TC
    tags:
    - native_custom_ops
ragged_attention:
  hammer_hstu:
    tags: []
  hstu:
    kernels:
    - _hstu_attn_fwd
    tags:
    - triton
  hstu_cuda:
    tags: []
rms_norm:
  aiter:
    tags: []
  liger_rms:
    kernels:
    - _rms_norm_forward_kernel
    - _block_rms_norm_forward_kernel
    tags:
    - triton
    - liger
  llama_rms:
    tags: []
  quack_rms:
    tags: []
  tilelang:
    kernels:
    - tritonbench.operators.rms_norm.tilelang.TileLangRMSNorm.forward
    tags:
    - tilelang
  torch_compile_rms:
    tags:
    - pt2
  triton_fused_rmsnorm:
    kernels:
    - _rms_norm_bwd_fused
    tags:
    - triton
rope:
  apply_rotary_pos_emb:
    tags: []
  liger_rotary_pos_emb:
    tags:
    - liger
    - triton
  torch_compile_rotary_pos_emb_full_op:
    tags:
    - pt2
softmax:
  naive_softmax:
    tags:
    - aten
  quack:
    kernels:
    - quack.softmax._softmax_fwd
    - quack.softmax._softmax_backward
    tags:
    - cutedsl
  torch_compile_softmax:
    tags:
    - pt2
  triton_softmax:
    kernels:
    - Operator.softmax_kernel
    tags:
    - triton
sum:
  torch_compile_sum:
    tags:
    - pt2
  torch_sum:
    tags: []
  triton_sum:
    kernels:
    - triton_sum_kernel_scalar_result
    tags:
    - triton
swiglu:
  liger_swiglu:
    tags:
    - liger
    - triton
  torch_compile_swiglu:
    tags:
    - pt2
  torch_swiglu:
    tags: []
template_attention:
  test_no_exp2:
    kernels:
    - triton_tem_fused_no_exp2
    tags:
    - triton
  test_with_exp2:
    kernels:
    - triton_tem_fused_with_exp2
    tags:
    - triton
test_op:
  test_op:
    tags: []
vector_add:
  torch_add:
    tags: []
  torch_compile_add:
    tags:
    - pt2
  triton_add:
    kernels:
    - triton_add_kernel
    tags:
    - triton
vector_exp:
  torch_compile_exp:
    tags:
    - pt2
  torch_exp:
    tags: []
  triton_exp:
    kernels:
    - triton_exp_kernel
    tags:
    - triton
welford:
  eager_layer_norm:
    tags:
    - aten
  test_no_welford:
    kernels:
    - torch.ops.inductor._reinterpret_tensor
    - triton_red_fused_native_layer_norm_no_welford
    tags:
    - native_custom_ops
    - triton
  torch_compile_welford:
    tags:
    - pt2
  triton_welford:
    kernels:
    - torch.ops.inductor._reinterpret_tensor
    - triton_red_fused_native_layer_norm_0
    - triton_red_fused_native_layer_norm_no_welford
    tags:
    - native_custom_ops
    - triton
